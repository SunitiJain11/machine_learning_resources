a. machine learning resources:
   udemy course 
`  https://www.geeksforgeeks.org/machine-learning/
   coursera courses 

b. machine learning project ideas list:
    https://study.com/academy/lesson/linear-regression-project-ideas.html (regression specifically)
    https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/ 
    https://data-flair.training/blogs/machine-learning-project-ideas/
    REgression:
      1. stock prices.
      2. image scan of brain (mri ) and tell if person is happy or sad
      3. house prices
      4. number of retweets.
    

c. whenever spyder says "data_set" is not present , run file by that gree arrow once, working directory might not be set for that file.

d. What is boosting algo?
    when the preidctors/classifiers are week, we combine then to form a stronog one. This is called boosting algo.
    https://data-flair.training/blogs/gradient-boosting-algorithm/
    
e. What is gradient decent?
    https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/?ref=rp
    it is change in a parameter with respct to the inputs.
    A gradient is basically the slope of a function; the degree of change of a parameter with the amount of change in another parameter. Mathematically, it can be described as the partial derivatives of a set of parameters with respect to its inputs. The more the gradient, the steeper the slope. Gradient Descent is a convex function.
Gradient Descent can be described as an iterative method which is used to find the values of the parameters of a function that minimizes the cost function as much as possible.

f. What is ensemble learning?
   when we combine two or more alogs or one algo repeatitively so that we arrive at a stringer conclusion , itis called ensemble learning.

1. data pre processing : (udemy)
https://hackernoon.com/what-steps-should-one-take-while-doing-data-preprocessing-502c993e1caa 

    1.1 for test- train split use sklearn.model_selection, not sklean.cross_validation.

    1.2 random state in test_train_split(), ensures that whenever you splt, the train-test set is same.
    
    Random state ensures that the splits that you generate are reproducible. Scikit-learn uses random permutations to    generate the splits. The random state that you provide is used as a seed to the random number generator. This ensures that the random numbers are generated in the same order.
    
    https://stackoverflow.com/questions/49147774/what-is-random-state-in-sklearn-model-selection-train-test-split-example/49147883#49147883?newreg=20d07e9d357644a2b6da4493338f2974
    
    1.3 fit_transform()
         always give an array of shape of column i.e.array=  array.resize(1,-1), not array=array.resize(-1,1).
         sinxe it only works column wise , not rows wise.

2. regression: (udemy)
 https://towardsdatascience.com/a-beginners-guide-to-linear-regression-in-python-with-scikit-learn-83a8f7ae2b4f  -for basics.
 
 2.0 What is R^2 or R square?
    in regressino we minimise (yi- y')^2, sum of this of all i from (1 to n) is Sres (residual).
    also lets find (yi-yavg)^2 sum called Stotal (arround the average.
    R-sq is (1- (Sres/ Stotal)
    closer R-sq to one, it is better. (i.e. Sres should be minimised).
    R-sq can be -ve too, i.e. our regression is worst fit.
    
    Adjusted R-square is modified R- sq. It takes into account for increasing number of variables. Since R-sq always increases with inc number of variable, adjusted R-sq takes care it happens in balalnced way. i.e. it might decrese if new varible is not important.
    It tells if added varible actuall is a good feature or not.
    It tells how good is our linen as compared to average line.
 2.1 no need of feature scaling, library already takes care.
 2.2 multiple regression : Backward elimination
    Statsmodel.api only not Statsmodel.formulae.api
 2.3 p value:
     let say I clain this feature has affect in data-> H1 , bu topposition says it has no affect->H0
     I will calculate the probsbilty of H0 happending say it P
     if P> some level(let ay significance level: alphs) : then H0 is true
     else H1 is true
     
     this p is "p value".
 2.4 regression types:
     https://www.listendata.com/2018/03/regression-analysis.html?m=1
 
 2.5 multiple regression types:
     https://en.m.wikipedia.org/wiki/Stepwise_regression 
     
 2.6 plynomial linear regression 
    2.6.1  it is called linear because the relation between y and coffiecients is linear. because at the end we need cofficients.  
     likt y = b0 + b1*x + b2*x^2 ..
     y is linearly depdendant on b0, b1, b2, etc.
     had it ben y= (b0+ x*b2)/(b3+ b4* x^4) something, then it would have been non linear.
    
    2.6.2 when dataset is small, eg, in our polynomial one, we don't split into train. we don't waste dataset that we have. We use it all so that we can have best prediction.
     
 2.7 gradient descent in regression
        a method to choose coefficients, so that is cost is minimum.
        https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/
        here theta1, theeta 2 are b0, b2 of regression equation.
 2.8 support vector regressor
    2.8.1 what is kernel 
        https://data-flair.training/blogs/svm-kernel-functions/ 
    2.8.2 svr class doesn't do feature scaling, we need to take care of that.  
    2.8.3 reshape in python
          https://www.geeksforgeeks.org/numpy-reshape-python/
          reshapes one array to another 
    2.8.4 what is reshape(-1,x)?
          reshape means given the rows find the number of columns it self.
          https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape
 2.9   Decision tree regression
          decision tree are of two types: classification or regression. It is more powerful in multidimentional.  

3. Classification
   it predicts the category of a obbject from given categorires.
   
   3.1 logistice regression - linear model
      3.1.0 what is math behind logistic regression.
      it tells the likely hoood. probabilty of a item for being in a class.
      mathematically
      y= b0 + x* b1 ...eq(1)
      then it applies sigmoid function to make in it range of 0 to 1 on y.
      p= 1/(1+ e^(-y))  ...eq(2)
      
      put value of y back into eq (1)
      log(p/(1-p))=  b0 + x* b1.
      the grapgh is thus transfromed.
      
      3.1.1 What is p(hat) p^ (actuall, is like i cap in vectors, x^, y^,z^)
      it denotes the probability of something which can b epredicted.   
      
   3.2 SVM - support vector machine 
      3.2.1 it used a kernel that transforms the non-linearly saperable data from n - dimension to (n+1)     dimensions. Then the data becomes saperable by line/ plane/ hyperplane.
      3.2.2 types of SVM -> gaussian ,sigmoidal, polunomial  etc.
      3.2.3 we can combine 2 svms if needed, one svm doesn't affect others, since points at distant values have function value '0'.
      3.2.3 various visualisation of different kernels: https://datafreakankur.com/machine-learning-kernel-functions-3d-visualization/
      http://www.cs.otago.ac.nz/staffpriv/mccane/publications/Szymanski2011Visualising.pdf
   
   3.3 Decision Tree
      3.3.1 you don;t ned to feature scale as the values/algorithm i snot based on euclidian distance!
      3.3.2 it may be prone to overfitting
   
   3.4 Naive bayse
      3.4.1 its called naive because it assumes all the variabls detemining the output are independant with respect to each other. it is an assumption. It may be incorrect in real world data.
      3.4.2 data given->age, salary , predict whether it will drive or walk.
            the probabilty to walk given X-> his features given of a person to determine result= p(walks|X) = p(X|walks) *p(walks)/ p(X)
            similarly the probabilty to drive = p(drives|X) = p(X|drives) *p(drives)/ p(X)
            for comparing these two: we have :  p(X|walks) *p(walks)/ p(X) vs p(X|drives) *p(drives)/ p(X)
            multiplying bith sides by P(x) we have  p(X|walks) *p(walks)vs p(X|drives) *p(drives)
            it is a famous result in mamy ML courses.
      
     
