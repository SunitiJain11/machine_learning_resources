a. machine learning resources:
   udemy course 
`  https://www.geeksforgeeks.org/machine-learning/
   coursera courses 

b. machine learning project ideas list:
    https://study.com/academy/lesson/linear-regression-project-ideas.html (regression specifically)
    https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/ 
    https://data-flair.training/blogs/machine-learning-project-ideas/
    REgression:
      1. stock prices.
      2. image scan of brain (mri ) and tell if person is happy or sad
      3. house prices
      4. number of retweets.
    

c. whenever spyder says "data_set" is not present , run file by that gree arrow once, working directory might not be set for that file.

d. What is boosting algo?
    when the preidctors/classifiers are week, we combine then to form a stronog one. This is called boosting algo.
    https://data-flair.training/blogs/gradient-boosting-algorithm/
    
e. What is gradient decent?
    https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/?ref=rp
    it is change in a parameter with respct to the inputs.
    A gradient is basically the slope of a function; the degree of change of a parameter with the amount of change in another parameter. Mathematically, it can be described as the partial derivatives of a set of parameters with respect to its inputs. The more the gradient, the steeper the slope. Gradient Descent is a convex function.
Gradient Descent can be described as an iterative method which is used to find the values of the parameters of a function that minimizes the cost function as much as possible.

f. What is ensemble learning? aka **stacking** 
   when we combine two or more alogs or one algo repeatitively so that we arrive at a stringer conclusion , itis called ensemble learning. used in random forest.
   
g. type 1 errro ->false opsitive: we predicted something me happe it didn;t. (pers note: think it as less dangerours, like we predicted a disaster and it didn't come, its okay)   
   type 2 error -> false negative.
   confusion matrix:
             y_predcted
               0   1
     y      0 TN   FP
     actual 1 FN   TP
     
     precision = (TP) / (TP+FP) --> rememberit as how many true positves i gave out of all positive predicted
     recall = (TP) / (TP+FN)   -->  rememberit as how many true positves i gave out of all actual positives
     correct = (TP+ Tn)/(total) ->  rememberit as how many correctly predicted
     error rate: 1-correct


f. accuracy paradox: https://medium.com/datadriveninvestor/accuracy-paradox-87b1dfe07ca7 
    y_predcted
               0   1
     y      0 9700 150
     actual 1 50   100 acuracy: 98%
     y_predcted
               0   1
     y      0 9850 0
     actual 1 150  0 acuracy: 98.5%
     predicting everything negative has more accuracy!!
 
 g. CAP_> cummulaitve accuray model is a beter approach for accuracy testing.
    https://medium.com/@lotass/classification-models-performance-evaluation-c3a91562793 
    see on curve what is your value of 50%,
    if X-> your model predction
      x<60 % ->rubbish
      x<70 % -> poor
      x<80 % ->good
      x<90 % -> very good
      x<100% -> too good, maybe overfiting
      
    the perfect line you see in cap curve , made up of 2 linear lines. It represents teh hypothetical situation, where if you alwasy pick right cusomer to advertise to, i.e. first 10% you choose was exactly same as those 10% you actaully brought the car!therefore it goes up like a rocket. Meett maxima and then stops , a staright horizontal line.  
      
 h. Bias and Variance
   https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229  
   
   bias -> diff bwt avg and predicted. 
    Model with high bias pays very little attention to the training data and oversimplifies the model.
    
    In supervised learning,** underfitting** happens when a model unable to capture the underlying pattern of the data. These models usually have **high bias and low variance** . It happens when we have very less amount of data to build an accurate model or when we try to build a linear model with a nonlinear data. Also, these kind of models are very simple to capture the complex patterns in data like Linear and logistic regression.
    
   In case of high bias we can:
   Try to add more features
   Try to make the model more complicated (add polynomial features)
   Try to fit the data better. apply regularization techniques (decrease lambda)
   
    
   Variance -> variation of predicted data
    Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasnâ€™t seen before . As a result, such models perform very well on training data but has high error rates on test data.
    
    In supervised learning, **overfitting happens** when our model captures the noise along with the underlying pattern in data. It happens when we train our   model a lot over noisy dataset. These models have ** low bias and high variance**. These models are very complex like Decision trees which are prone to overfitting.

Some of the most common techniques to resolve cases of high Variance would be:
add more training examples.
Try smaller sets of features (because you are overfitting). 
Try dimensionality reduction techniques. 
Get rid of irrelevant/redundant features.
Use Regularization techniques to prevent model overfitting. (increase lambda)



1. data pre processing : (udemy)
https://hackernoon.com/what-steps-should-one-take-while-doing-data-preprocessing-502c993e1caa 

    1.1 for test- train split use sklearn.model_selection, not sklean.cross_validation.

    1.2 random state in test_train_split(), ensures that whenever you splt, the train-test set is same.
    
    Random state ensures that the splits that you generate are reproducible. Scikit-learn uses random permutations to    generate the splits. The random state that you provide is used as a seed to the random number generator. This ensures that the random numbers are generated in the same order.
    
    https://stackoverflow.com/questions/49147774/what-is-random-state-in-sklearn-model-selection-train-test-split-example/49147883#49147883?newreg=20d07e9d357644a2b6da4493338f2974
    
    1.3 fit_transform()
         always give an array of shape of column i.e.array=  array.resize(1,-1), not array=array.resize(-1,1).
         sinxe it only works column wise , not rows wise.

2. regression: (udemy)
 https://towardsdatascience.com/a-beginners-guide-to-linear-regression-in-python-with-scikit-learn-83a8f7ae2b4f  -for basics.
 
 2.0 What is R^2 or R square?
    in regressino we minimise (yi- y')^2, sum of this of all i from (1 to n) is Sres (residual).
    also lets find (yi-yavg)^2 sum called Stotal (arround the average.
    R-sq is (1- (Sres/ Stotal)
    closer R-sq to one, it is better. (i.e. Sres should be minimised).
    R-sq can be -ve too, i.e. our regression is worst fit.
    
    Adjusted R-square is modified R- sq. It takes into account for increasing number of variables. Since R-sq always increases with inc number of variable, adjusted R-sq takes care it happens in balalnced way. i.e. it might decrese if new varible is not important.
    It tells if added varible actuall is a good feature or not.
    It tells how good is our linen as compared to average line.
 2.1 no need of feature scaling, library already takes care.
 2.2 multiple regression : Backward elimination
    Statsmodel.api only not Statsmodel.formulae.api
 2.3 p value:
     let say I clain this feature has affect in data-> H1 , bu topposition says it has no affect->H0
     I will calculate the probsbilty of H0 happending say it P
     if P> some level(let ay significance level: alphs) : then H0 is true
     else H1 is true
     
     this p is "p value".
 2.4 regression types:
     https://www.listendata.com/2018/03/regression-analysis.html?m=1
 
 2.5 multiple regression types:
     https://en.m.wikipedia.org/wiki/Stepwise_regression 
     
 2.6 plynomial linear regression 
    2.6.1  it is called linear because the relation between y and coffiecients is linear. because at the end we need cofficients.  
     likt y = b0 + b1*x + b2*x^2 ..
     y is linearly depdendant on b0, b1, b2, etc.
     had it ben y= (b0+ x*b2)/(b3+ b4* x^4) something, then it would have been non linear.
    
    2.6.2 when dataset is small, eg, in our polynomial one, we don't split into train. we don't waste dataset that we have. We use it all so that we can have best prediction.
     
 2.7 gradient descent in regression
        a method to choose coefficients, so that is cost is minimum.
        https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/
        here theta1, theeta 2 are b0, b2 of regression equation.
 2.8 support vector regressor
    2.8.1 what is kernel 
        https://data-flair.training/blogs/svm-kernel-functions/ 
    2.8.2 svr class doesn't do feature scaling, we need to take care of that.  
    2.8.3 reshape in python
          https://www.geeksforgeeks.org/numpy-reshape-python/
          reshapes one array to another 
    2.8.4 what is reshape(-1,x)?
          reshape means given the rows find the number of columns it self.
          https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape
 2.9   Decision tree regression
          decision tree are of two types: classification or regression. It is more powerful in multidimentional.  

3. Classification
   it predicts the category of a obbject from given categorires.
   
   3.1 logistice regression - linear model
      3.1.0 what is math behind logistic regression.
      it tells the likely hoood. probabilty of a item for being in a class.
      mathematically
      y= b0 + x* b1 ...eq(1)
      then it applies sigmoid function to make in it range of 0 to 1 on y.
      p= 1/(1+ e^(-y))  ...eq(2)
      
      put value of y back into eq (1)
      log(p/(1-p))=  b0 + x* b1.
      the grapgh is thus transfromed.
      
      3.1.1 What is p(hat) p^ (actuall, is like i cap in vectors, x^, y^,z^)
      it denotes the probability of something which can b epredicted.   
      
   3.2 SVM - support vector machine 
      3.2.1 it used a kernel that transforms the non-linearly saperable data from n - dimension to (n+1)     dimensions. Then the data becomes saperable by line/ plane/ hyperplane.
      3.2.2 types of SVM -> gaussian ,sigmoidal, polunomial  etc.
      3.2.3 we can combine 2 svms if needed, one svm doesn't affect others, since points at distant values have function value '0'.
      3.2.3 various visualisation of different kernels: https://datafreakankur.com/machine-learning-kernel-functions-3d-visualization/
      http://www.cs.otago.ac.nz/staffpriv/mccane/publications/Szymanski2011Visualising.pdf
   
   3.3 Decision Tree
      3.3.1 you don;t ned to feature scale as the values/algorithm i snot based on euclidian distance!
      3.3.2 it may be prone to overfitting
   
   3.4 Naive bayse
      3.4.1 its called naive because it assumes all the variabls detemining the output are independant with respect to each other. it is an assumption. It may be incorrect in real world data.
      3.4.2 data given->age, salary , predict whether it will drive or walk.
            the probabilty to walk given X-> his features given of a person to determine result= p(walks|X) = p(X|walks) *p(walks)/ p(X)
            similarly the probabilty to drive = p(drives|X) = p(X|drives) *p(drives)/ p(X)
            for comparing these two: we have :  p(X|walks) *p(walks)/ p(X) vs p(X|drives) *p(drives)/ p(X)
            multiplying bith sides by P(x) we have  p(X|walks) *p(walks)vs p(X|drives) *p(drives)
            it is a famous result in mamy ML courses.
      
     
